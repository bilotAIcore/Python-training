{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"py-logo.png\" width=\"100pt\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# PYTHON FOR DATA SCIENCE \n",
    "# VI â€“ NEURAL NETWORKS\n",
    "*Lasse Ruokolainen*\n",
    "\n",
    "*Seasoned Data Master, BILOT Consulting Oy* \n",
    "\n",
    "***\n",
    "\n",
    "## (1) Data preparation\n",
    "When using neural networks, certain data preparation is typically required. Data preparation is an important step in modeling the neural network. The procedure for the preparation of the data affects many important parameters. It reduces the modeling errors, speeds up the process of training the neural network and leads to simplification of the system as a whole. What is typically done is that categorical features are either label- or onehot encoded and numerical features are scaled.\n",
    "\n",
    "### (1.1) Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the example data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Titanic dataset\n",
    "df = pd.read_csv('Datasets/titanic_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function for making necessary preprocessing, so that we can easily process new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess(data, columns_to_ignore, columns_to_scale):\n",
    "    \n",
    "    # Delete columns\n",
    "    data.drop(columns_to_ignore, axis = 1, inplace=True)\n",
    "    \n",
    "    # Numeric encoding of \"sex\":\n",
    "    data.sex = np.where(data.sex == 'female',1,0)\n",
    "    \n",
    "    # Scale values:\n",
    "    data[columns_to_scale] = data[columns_to_scale].apply(lambda x: (x-min(x))/max(x))\n",
    "        \n",
    "    return data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we cam apply the function to our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ignore 'name' and 'ticket':\n",
    "to_ignore = [\"name\", \"ticket\"]\n",
    "\n",
    "# Scale 'age' and 'fare':\n",
    "to_scale = [\"age\", \"fare\"]\n",
    "\n",
    "# Preprocess data\n",
    "X = preprocess(df.drop('survived',axis=1), to_ignore, to_scale)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Data splitting\n",
    "\n",
    "Of course, we need to split the data, to be able to judge the reliability of our model. Here a challenge is that we have quite a small sample in our disposal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, df.survived.values, \n",
    "    test_size = 0.10, random_state = 123\n",
    ")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Modeliing\n",
    "\n",
    "### (2.1) Simple MLP\n",
    "\n",
    "MLP stands for *Milty Layer Perceptron*\n",
    "\n",
    "#### (a) *Model fitting*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes = (11,7,),\n",
    "                    verbose = True, batch_size = 16,\n",
    "                    max_iter = 100,learning_rate='adaptive',\n",
    "                    early_stopping=True, activation='relu')\n",
    "\n",
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(mlp.loss_curve_,'.-',markersize = 10)\n",
    "plt.ylabel(\"Training loss\")\n",
    "plt.xlabel(\"Training epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy: %.2f \\n' %mlp.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) *Model evaluation*\n",
    "\n",
    "This level of training accuracy does not indicate that the model would be over-fitting. \n",
    "Still, we need to see how well the model performs against the testing data. The `classification_report` function from `sklearn.metrics` is quite usefull:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Calculate model predictions:\n",
    "preds = mlp.predict(X_test)\n",
    "\n",
    "print('Test Accuracy: %.2f \\n' %mlp.score(X_test,y_test))\n",
    "\n",
    "print('Classification report:')\n",
    "print(classification_report(y_true = y_test, y_pred = preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the rather small amount of training data (a bit over 1000 records), the model is performing surprisingly well.\n",
    "\n",
    "Let's plot the ROC curve also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "predsp = mlp.predict_proba(X_test)[:,1]\n",
    "auc = round(roc_auc_score(y_true=y_test, y_score=predsp),2)\n",
    "fpr, tpr, ths = roc_curve(y_true=y_test, y_score=predsp,pos_label=1)\n",
    "\n",
    "plt.plot(fpr,tpr)\n",
    "plt.plot([0,1],[0,1],'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.title('AUC: {}'.format(auc))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This analysis indicates that the model is able to differentiate between survivers and non-survivers with 82% probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Keras deep learning\n",
    "\n",
    "Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
    "\n",
    "In other words, it is much more flexible than the `MLPClassifier` in `sklearn`. Still one does not need to program everything from scratch.\n",
    "\n",
    "#### (a) *Model building*\n",
    "With Keras one needs to build the network architecture explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "dnn = Sequential()\n",
    "dnn.add(Dense(11, input_dim = 6,  kernel_initializer='uniform', activation='relu'))\n",
    "dnn.add(Dense(7, input_dim = 6,  kernel_initializer='uniform', activation='relu'))\n",
    "dnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "dnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) *Model fitting*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = dnn.fit(X_train, y_train, \n",
    "                    validation_data = (X_test, y_test), \n",
    "                    epochs = 10, batch_size = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize training loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = list(range(1,21))\n",
    "H = pd.DataFrame(history.history)\n",
    "\n",
    "plt.plot(epochs,H['loss'],'.-',label='train')\n",
    "plt.plot(epochs,H['val_loss'],'r.-',label='test')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) *Model evaluation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Calculate model predictions:\n",
    "preds = dnn.predict_classes(X_test)\n",
    "\n",
    "print('Test Accuracy: %.2f \\n' %accuracy_score(y_test,preds))\n",
    "\n",
    "print('Classification report:')\n",
    "print(classification_report(y_true = y_test, y_pred = preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very unlikely that the result will vary significantly between the two models implemented here, due to the small amount of data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLtesting",
   "language": "python",
   "name": "mltesting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "261.818px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
