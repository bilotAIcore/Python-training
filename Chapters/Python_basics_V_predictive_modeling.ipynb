{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"py-logo.png\" width=\"100pt\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# PYTHON FOR DATA SCIENCE \n",
    "# V â€“ PREDICTIVE MODELING\n",
    "*Lasse Ruokolainen*\n",
    "\n",
    "*Seasoned Data Master, BILOT Consulting Oy* \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (1) HOLDOUT METHOD\n",
    "When training predictive models, it is common practice to *split* the data and use only one part for the training. This is done to facilitate the evaluation of the generality of the model. As an example, we will here use a data set that contains information on credit defaults. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) *Data preparation*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('Datasets/german_credit_data.csv', sep=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there are several missing values in two categorical features. A good way to proceed is to replace them with 'missing': "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed = df.fillna('missing')\n",
    "print(df_imputed.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to make the response variable categorical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed['Default'] = df_imputed['Default'].map({1:'no',2:'yes'})\n",
    "print(df_imputed['Default'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data is in order, we need to still transform it in a form useable to `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response / label:\n",
    "Y = df_imputed['Default']\n",
    "# predictors:\n",
    "X = df_imputed.drop('Default',axis=1)\n",
    "\n",
    "# convert factor to binary form:\n",
    "X = pd.get_dummies(X,drop_first=True)\n",
    "\n",
    "X.head()\n",
    "print(df_imputed.shape)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) *Splitting the data*\n",
    "Having the data prepared, we can now procede to splitting the data. While there is now definite rule for choosing the splitting proportion, usually a fraction between 0.7-0.8 is used for the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a random training set:\n",
    "import numpy as np\n",
    "from random import sample\n",
    "\n",
    "p_train = 0.8 # proportion of examples allocated to training\n",
    "indices = sample(range(X.shape[0]),round(X.shape[0] * p_train))\n",
    "indices[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split and convert to array:\n",
    "X_train = X.iloc[indices,:].values\n",
    "X_test = X.drop(indices).values\n",
    "\n",
    "Y_train = Y.iloc[indices].values\n",
    "Y_test = Y.drop(indices).values\n",
    "\n",
    "# print the resulting proportions:\n",
    "print('Proportion training: %.2f' %(X_train.shape[0]/X.shape[0]))\n",
    "print('Proportion testing: %.2f' %(X_test.shape[0]/X.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good stuff, we have managed make a random split of the data, and the size of the training and testing sets match with what we expected to get. We are now all set for building some predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) MODEL TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will try to build a model that is able to predict the response variabe, with an acceptable accuracy. We have alrewady prepared the data and we are ready to proceed. All we need now is to load some methods from `sklearn` and put them into use. Remember that in the example data, the response variable is categorical (yes/no), which means that we are faced with a *classification problem*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) *Logistic regression*\n",
    "Do not overlook the utility of simple methods. Logistic regression is an exellent method for benchmarking predictability, as there is practically no risk of overfitting and you also get interpretable estimates of the role of different predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the logistic regression model:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# define a model object:\n",
    "model_logistic = LogisticRegression(solver = 'lbfgs',max_iter=200)\n",
    "\n",
    "# fit the model using the training data:\n",
    "model_logistic.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the model trained, we need to evaluate how well the model performs in classifying the response variable. This we do by predicting the outcome, using the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model_logistic.score(X_train,Y_train)\n",
    "print('Accuracy: %.2f' %result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, now we have something to compare to. \n",
    "\n",
    "We can also visualise the model coefficients, as we did in the **Statistics** section. This can be usefull when trying to understand which factors most strongly affect the outcome and whether their effect is positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot model coefficients:\n",
    "fig, ax = plt.subplots(figsize=(7,6))\n",
    "ax.barh(X.columns,model_logistic.coef_[0,:],orientation='horizontal')\n",
    "ax.invert_yaxis()\n",
    "ax.grid(color='gray', linestyle='-', linewidth=0.5,axis='y')\n",
    "ax.set_axisbelow(True)\n",
    "plt.title('Feature coefficients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) *Decision tree classifier*\n",
    "\n",
    "Decision Trees are a non-parametric (supervised learning) methods, used for classification and regression. The goal is to create a model that predicts the value of a responce variable by learning simple decision rules inferred from the data features. What is good about them is that they are simple to understand and to interpret and the trees can be visualised. On the down side, decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting (we'll get back to this later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "\n",
    "model_tree = DT(criterion='entropy',max_depth=12)\n",
    "\n",
    "model_tree.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model_tree.score(X_train,Y_train)\n",
    "print('Accuracy: %.2f' %result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! This looks amazing, we were able to correctly classify nearly every example. But, recall that decision trees easily overfitt the data, which means we should be cautious here.\n",
    "\n",
    "The decision tree classifier also provides an estima of feature importances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importances:\n",
    "fig, ax = plt.subplots(figsize=(4,7))\n",
    "ax.barh(X.columns,model_tree.feature_importances_,orientation='horizontal')\n",
    "ax.grid(color='gray', linestyle='-', linewidth=0.5,axis='y')\n",
    "ax.set_axisbelow(True)\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Feature importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree topology can be visualised as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "from sklearn.externals.six import StringIO \n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image \n",
    "import pydotplus\n",
    "\n",
    "dot_data = StringIO()\n",
    "export_graphviz(model_tree, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,feature_names=X.columns)\n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue()) \n",
    "Image(graph.create_png())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph is quite big, which is due to the fact that categorical features have been binary-coded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) MODEL EVALUATION\n",
    "We start by producing model predictions (i.e., the predicted outcome) using the training set. In order to reduce the amount of code, the results for each model will be stored in a dictionary, from where they can be easily accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "\n",
    "# generate model predictions for training and store them in a dictionary:\n",
    "pred_tr = dict()\n",
    "pred_tr['logistic'] = model_logistic.predict(X_train)\n",
    "pred_tr['tree'] = model_tree.predict(X_train)\n",
    "\n",
    "pred_tr_p = dict()\n",
    "pred_tr_p['logistic'] = model_logistic.predict_proba(X_train)[:,1]\n",
    "pred_tr_p['tree'] = model_tree.predict_proba(X_train)[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) *Confusion matrix*\n",
    "Having the model predictions ready, we can calculate the *confusion matrix* for each model, by looping through the dictionary of predicted values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the confusion matrices:\n",
    "conf = dict()\n",
    "for i in pred_tr.keys():\n",
    "    conf[i] = confusion_matrix(Y_train,pred_tr[i])\n",
    "\n",
    "print('Logistic regression: \\n',conf['logistic'], '\\n')\n",
    "print('Decision tree: \\n',conf['tree'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix is a simpke cross-tabulation of the *observed* and *predicted* outcomes. On the diagonal we have correctly classified examples (i.e., observed and predicted match) and on the off-diagonal we have misclassified examples. Try calculate model accuracies yourself, using the confusion matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) *Receiver Operator Characteristic*\n",
    "\n",
    "The following code calculates the *false positive rate* and *true positive rate*, for different cut-off points along predicted class probabilities, for each model. Within the for-loop, also *AUC* (Area Under Curve) metric is calculated, based on the `fpr` and `tpr` values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr,thresholds = dict(), dict(), dict()\n",
    "roc_auc = dict()\n",
    "for i in pred_tr.keys():\n",
    "    fpr[i], tpr[i], thresholds[i] = roc_curve(Y_train, pred_tr_p[i], pos_label='yes')\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i]).round(3)\n",
    "\n",
    "print(roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When these values have been obtained, we can plot the *ROC curves* for each model. The `enumerate` function is very convenient, as it returns the input values along with their index locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,4))\n",
    "for ind,i in enumerate(fpr.keys()):\n",
    "    plt.subplot(1,2,ind+1)\n",
    "    plt.plot(fpr[i], tpr[i], color='darkorange',\n",
    "         lw=2, label='ROC curve (area = %0.2f)' % roc_auc[i])\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(i)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue dashed lines in the plots indicate the case of fully random prediction, i.e., coin tossing; the higher the orange line sits above this reference line, the better the model is in differentiating between outcomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) *Model testing*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model_logistic.score(X_test,Y_test), model_tree.score(X_test,Y_test)\n",
    "\n",
    "print('Logistic model, testing accuracy: %.2f' %result[0])\n",
    "print('Decision tree model, testing accuracy: %.2f' %result[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like our decision tree model was not that good after all, whereas the accuracy is practically the same for logistic regression. Let's next compare the *ROC curves* between data sets: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate model predictions for testing and store them in a dictionary:\n",
    "pred_ts = dict()\n",
    "pred_ts['logistic'] = model_logistic.predict_proba(X_test)[:,1]\n",
    "pred_ts['tree'] = model_tree.predict_proba(X_test)[:,1]\n",
    "\n",
    "fpr2, tpr2,thresholds2 = dict(), dict(), dict()\n",
    "roc_auc2 = dict()\n",
    "for i in pred_ts.keys():\n",
    "    fpr2[i], tpr2[i], thresholds2[i] = roc_curve(Y_test, pred_ts[i], \n",
    "                                                 pos_label='yes')\n",
    "    roc_auc2[i] = auc(fpr2[i], tpr2[i]).round(3)\n",
    "\n",
    "print(roc_auc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot ROC curves for both training and testing data:\n",
    "plt.figure(figsize=(9,4))\n",
    "for ind,i in enumerate(fpr.keys(),1):\n",
    "    plt.subplot(1,2,ind)\n",
    "    plt.plot(fpr2[i], tpr2[i], color='darkorange',\n",
    "         lw=2, label='Testing (AUC = %0.2f)' % roc_auc2[i])\n",
    "    plt.plot(fpr[i], tpr[i], color='red',\n",
    "         lw=2, label='Training (AUC = %0.2f)' % roc_auc[i])    \n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(i)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation here is that the tree model is overfitting the training data, while the logistic model is not.\n",
    "\n",
    "### (d) *Classification report*\n",
    "A very nice test output is provided by the `classification_report` function from `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\n",
    "    classification_report(\n",
    "        y_true = Y_test, \n",
    "        y_pred = model_logistic.predict(X_test)\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLtesting",
   "language": "python",
   "name": "mltesting"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
